---
title: Information vs. Entropy vs. Meaning
created: Thu Apr 01 2021 02:41:53 GMT-0700 (Pacific Daylight Time)
modified: Thu Apr 01 2021 02:41:53 GMT-0700 (Pacific Daylight Time)
---
# Information vs. Entropy vs. Meaning

information = -(entropy)
entropy = -(information)

Information is the possibility of meaning. The expectant space for a 1 or a 0, or a 'q', or a bird, or a line from a poem, or a profound insight like "time is fleeting".

Etymologically it makes sense: in- ("the absence of") -formation ("patterned structure"). Framed this way, information is a vehicle that awaits, accepts, and transports meaning. 

But there is another perspective. Mathematically, entropy was invented to explain death, or, in the terms physicians prefer, the loss of heat in the universe. From a UX perspective, entropy is the loss of the possibility of meaning, the decay of the vehicle in which meaning could ride. For a user, entropy is missing out.

Physicists tell us that entropy rises in a system as that system cools. Less untried random combinations and permutations exist, less vibrations can result in creations, less evolutions can occur.

This seemed simple when I set out, but this is some seriously obtuse philosophical crap. I'd be loathe to let you languor in the lurch without some illustrative creature comforts. And I certainly can't profess to having the ultimate answer to such a deeply misunderstood concept. However, I will posit my take on the matter.

## Here is a once-relevant example of information vs. entropy vs. meaning

### Information
Everything that's been reported about the disappearance of Malaysian Airlines Flight 370

### Entropy
All of the incorrect meanings carried as information cool each other off.

### Meaning
You stand a minuscule chance of being inexplicably lost on the planet when you board plane.

## What does it all mean?

A user can find the correct meaning eventually by waiting for information on the matter to congeal. But a more magnified examination of the matter can speed coagulation.

It occurs to me that if information is mathematically the opposite of entropy, there lies much to be learned about 
information.

information = -(entropy)
entropy = -(information)

Philosophically speaking then, to maximize the meaning you culture, you would want to minimize the signal noise in your life. In other words, fight entropy at a basic level and you stand a better chance of defeating it at higher levels.